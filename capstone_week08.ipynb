{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9b12c32-9a98-4bb2-a42d-ff3d3ea1963c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Loaded master data with 136 total rows.\n",
      "INFO: \n",
      "--- Processing Function F1 (2D) ---\n",
      "INFO: Function F1: Loaded 16 clean points. Dimension D=2 enforced. f_best=0.1744\n",
      "INFO: GPR Model Trained. Log-Marginal-Likelihood: -21.9488\n",
      "INFO: F1 Proposed Query (D=2): [0.662067, 0.47948] (Rounded to 6 DP)\n",
      "INFO: \n",
      "--- Processing Function F2 (2D) ---\n",
      "INFO: Function F2: Loaded 15 clean points. Dimension D=2 enforced. f_best=0.6660\n",
      "INFO: GPR Model Trained. Log-Marginal-Likelihood: -15.8999\n",
      "INFO: F2 Proposed Query (D=2): [0.769782, 1.0] (Rounded to 6 DP)\n",
      "INFO: \n",
      "--- Processing Function F3 (3D) ---\n",
      "INFO: Function F3: Loaded 14 clean points. Dimension D=3 enforced. f_best=-0.0200\n",
      "INFO: GPR Model Trained. Log-Marginal-Likelihood: -18.7991\n",
      "INFO: F3 Proposed Query (D=3): [0.664275, 0.600999, 0.535155] (Rounded to 6 DP)\n",
      "INFO: \n",
      "--- Processing Function F4 (4D) ---\n",
      "INFO: Function F4: Loaded 15 clean points. Dimension D=4 enforced. f_best=-2.9741\n",
      "INFO: GPR Model Trained. Log-Marginal-Likelihood: -14.8504\n",
      "INFO: F4 Proposed Query (D=4): [0.428378, 0.369407, 0.345557, 0.399058] (Rounded to 6 DP)\n",
      "INFO: \n",
      "--- Processing Function F5 (4D) ---\n",
      "INFO: Function F5: Loaded 15 clean points. Dimension D=4 enforced. f_best=1645.2033\n",
      "INFO: GPR Model Trained. Log-Marginal-Likelihood: -15.3968\n",
      "INFO: F5 Proposed Query (D=4): [1.0, 1.0, 1.0, 1.0] (Rounded to 6 DP)\n",
      "INFO: \n",
      "--- Processing Function F6 (5D) ---\n",
      "INFO: Function F6: Loaded 15 clean points. Dimension D=5 enforced. f_best=-0.7143\n",
      "INFO: GPR Model Trained. Log-Marginal-Likelihood: -20.5566\n",
      "INFO: F6 Proposed Query (D=5): [0.510625, 0.33055, 0.518944, 0.77119, 0.154218] (Rounded to 6 DP)\n",
      "INFO: \n",
      "--- Processing Function F7 (6D) ---\n",
      "INFO: Function F7: Loaded 15 clean points. Dimension D=6 enforced. f_best=1.6487\n",
      "INFO: GPR Model Trained. Log-Marginal-Likelihood: -21.2841\n",
      "INFO: F7 Proposed Query (D=6): [0.967968, 0.468373, 0.152233, 0.998512, 0.040256, 0.912553] (Rounded to 6 DP)\n",
      "INFO: \n",
      "--- Processing Function F8 (8D) ---\n",
      "INFO: Function F8: Loaded 13 clean points. Dimension D=8 enforced. f_best=9.9951\n",
      "INFO: GPR Model Trained. Log-Marginal-Likelihood: -18.3404\n",
      "INFO: F8 Proposed Query (D=8): [0.128465, 0.244099, 0.0, 0.310018, 0.535545, 0.241708, 0.363253, 0.467537] (Rounded to 6 DP)\n",
      "INFO: \n",
      "--------------------------------------------------\n",
      "INFO: SUCCESS: Generated 8 FINAL queries for Week 8.\n",
      "INFO: File saved to: add_data/week08_clean_inputs.json\n",
      "INFO: This is your submission file: week08_clean_inputs.json\n",
      "INFO: --------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Queries for Week 08 Submission (week08_clean_inputs.json) ---\n",
      "F1: [0.662067, 0.47948]\n",
      "F2: [0.769782, 1.0]\n",
      "F3: [0.664275, 0.600999, 0.535155]\n",
      "F4: [0.428378, 0.369407, 0.345557, 0.399058]\n",
      "F5: [1.0, 1.0, 1.0, 1.0]\n",
      "F6: [0.510625, 0.33055, 0.518944, 0.77119, 0.154218]\n",
      "F7: [0.967968, 0.468373, 0.152233, 0.998512, 0.040256, 0.912553]\n",
      "F8: [0.128465, 0.244099, 0.0, 0.310018, 0.535545, 0.241708, 0.363253, 0.467537]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import Bounds\n",
    "import logging\n",
    "import warnings\n",
    "import json\n",
    "import os\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "# Import necessary components from scikit-learn for GPR\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, Matern, WhiteKernel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"The optimal value found for the log-marginal-likelihood is close to the specified upper bound\")\n",
    "\n",
    "# --- CONFIGURATION FOR WEEK 08 ---\n",
    "MASTER_FILE_PATH = 'bbo_master_w08.csv' # Loading the newly aggregated master file\n",
    "OUTPUTS_DIR = 'add_data'\n",
    "OUTPUT_FILENAME = 'week08_clean_inputs.json' # Generating queries for Week 08 submission\n",
    "NUM_FUNCTIONS = 8\n",
    "N_RESTARTS = 10 # Number of restarts for acquisition function optimization\n",
    "XI = 0.01 # The exploration trade-off parameter (standard for EI)\n",
    "\n",
    "# Function dimensionality mapping\n",
    "FUNCTION_DIMS = {\n",
    "    1: 2, 2: 2, 3: 3, 4: 4, 5: 4, 6: 5, 7: 6, 8: 8\n",
    "}\n",
    "# Search Bounds (all functions are constrained to the [0, 1] hypercube)\n",
    "LOWER_BOUND = 0.0\n",
    "UPPER_BOUND = 1.0\n",
    "\n",
    "def expected_improvement(X: np.ndarray, gp_model: GaussianProcessRegressor, f_best: float, xi: float = 0.01) -> np.ndarray:\n",
    "    \"\"\"Expected Improvement acquisition function for MAXIMISATION.\"\"\"\n",
    "    # X must be reshaped if it's a single point (1D -> 2D)\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape(1, -1)\n",
    "\n",
    "    # Get mean and standard deviation from the Gaussian Process model\n",
    "    mu, sigma = gp_model.predict(X, return_std=True)\n",
    "\n",
    "    # Suppress warnings for division by zero (sigma=0) if the GP is highly certain\n",
    "    with np.errstate(divide='ignore'):\n",
    "        # Calculate Z (the standard deviation-normalized difference from f_best)\n",
    "        Z = (mu - f_best - xi) / sigma\n",
    "\n",
    "        # Expected Improvement formula for MAXIMISATION:\n",
    "        # EI = (mu - f_best - xi) * Phi(Z) + sigma * phi(Z)\n",
    "        # EI is a measure of the expected gain over the current best (f_best).\n",
    "        ei = (mu - f_best - xi) * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "\n",
    "        # If sigma is zero, EI must be zero (we are already certain)\n",
    "        ei[sigma == 0.0] = 0.0\n",
    "\n",
    "    # We want to minimize the negative EI to maximize EI\n",
    "    return -ei\n",
    "\n",
    "def train_and_query_bbo() -> List[List[float]]:\n",
    "    \"\"\"Main function to run the BBO strategy for all 8 functions.\"\"\"\n",
    "\n",
    "    if not os.path.exists(MASTER_FILE_PATH):\n",
    "        logging.critical(f\"FATAL ERROR: Master file '{MASTER_FILE_PATH}' not found. Please run create_bbo_master_w08.py first.\")\n",
    "        return []\n",
    "\n",
    "    df_master = pd.read_csv(MASTER_FILE_PATH)\n",
    "    logging.info(f\"Loaded master data with {len(df_master)} total rows.\")\n",
    "\n",
    "    final_queries = []\n",
    "\n",
    "    # Use the robust Matern kernel with noise modeling (WhiteKernel) - Critical for noisy LLM outputs.\n",
    "    # Matern(nu=2.5) provides a good balance between RBF (smooth) and Exponential (non-smooth).\n",
    "    kernel = C(1.0, (1e-3, 1e3)) * Matern(length_scale=1.0, length_scale_bounds=(1e-3, 1e3), nu=2.5) + \\\n",
    "             WhiteKernel(noise_level=0.1, noise_level_bounds=(1e-5, 1e-1))\n",
    "\n",
    "    # --- Iterate through all 8 functions ---\n",
    "    for f_id in range(1, NUM_FUNCTIONS + 1):\n",
    "        f_name = f'F{f_id}'\n",
    "        dim = FUNCTION_DIMS[f_id]\n",
    "        # Filter for the current function and ensure Y values are present\n",
    "        df_f = df_master[df_master['Function ID'] == f_id].dropna(subset=['Y']).copy()\n",
    "\n",
    "        # 1. Prepare Data\n",
    "        x_cols = [f'X{d+1}' for d in range(dim)]\n",
    "        df_f.dropna(subset=x_cols, inplace=True)\n",
    "\n",
    "        X_raw = df_f[x_cols].values\n",
    "        Y_raw = df_f['Y'].values.reshape(-1, 1)\n",
    "\n",
    "        if len(X_raw) < 1:\n",
    "            logging.warning(f\"Skipping {f_name}: No complete data points found.\")\n",
    "            final_queries.append([np.nan] * dim)\n",
    "            continue\n",
    "\n",
    "        # Find the best observed Y value (f_best) for EI calculation (Maximisation problem)\n",
    "        f_best_raw = Y_raw.max()\n",
    "        logging.info(f\"\\n--- Processing Function {f_name} ({dim}D) ---\")\n",
    "        logging.info(f\"Function {f_name}: Loaded {len(X_raw)} clean points. Dimension D={dim} enforced. f_best={f_best_raw:.4f}\")\n",
    "\n",
    "        # 2. Scale Data\n",
    "        x_scaler = StandardScaler()\n",
    "        X_scaled = x_scaler.fit_transform(X_raw)\n",
    "\n",
    "        # 3. Train Gaussian Process Model\n",
    "        # normalize_y=True centers the output data, improving stability\n",
    "        gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, normalize_y=True, random_state=42)\n",
    "        gp.fit(X_scaled, Y_raw)\n",
    "        logging.info(f\"GPR Model Trained. Log-Marginal-Likelihood: {gp.log_marginal_likelihood_value_:.4f}\")\n",
    "\n",
    "        # 4. Find Next Query Point by maximizing the EI acquisition function\n",
    "        \n",
    "        # Acquisition optimization wrapper: converts unscaled X to scaled X for GP\n",
    "        def acquisition_wrapper(x_unscaled):\n",
    "            # The optimization operates on the unscaled space [0, 1]\n",
    "            x_scaled = x_scaler.transform(x_unscaled.reshape(1, -1))\n",
    "            # Evaluate EI using the scaled point and the raw f_best\n",
    "            return expected_improvement(x_scaled, gp, f_best_raw, XI)\n",
    "\n",
    "        # --- FIX: Ensure Bounds object is created with arrays, not a 'shape' argument ---\n",
    "        lower_bounds = np.full(dim, LOWER_BOUND)\n",
    "        upper_bounds = np.full(dim, UPPER_BOUND)\n",
    "        bounds_unscaled = Bounds(lower_bounds, upper_bounds)\n",
    "        # ---------------------------------------------------------------------------------\n",
    "\n",
    "        best_ei_value = np.inf\n",
    "        best_x_unscaled = None\n",
    "\n",
    "        # Use multiple random restarts in the unscaled space [0, 1]\n",
    "        for _ in range(N_RESTARTS):\n",
    "            x0_unscaled = np.random.uniform(LOWER_BOUND, UPPER_BOUND, size=dim)\n",
    "            # Use L-BFGS-B method for bounded optimization\n",
    "            res = minimize(acquisition_wrapper, x0=x0_unscaled, bounds=bounds_unscaled, method='L-BFGS-B')\n",
    "\n",
    "            if res.fun < best_ei_value:\n",
    "                best_ei_value = res.fun\n",
    "                best_x_unscaled = res.x\n",
    "\n",
    "        # Final check and formatting\n",
    "        if best_x_unscaled is not None:\n",
    "            query_point = best_x_unscaled.tolist()\n",
    "            # Round to 6 decimal places for clean submission\n",
    "            query_point_clean = [round(x, 6) for x in query_point]\n",
    "            final_queries.append(query_point_clean)\n",
    "            logging.info(f\"F{f_id} Proposed Query (D={dim}): {query_point_clean} (Rounded to 6 DP)\")\n",
    "        else:\n",
    "            # Fallback to a random point if optimization fails\n",
    "            random_point = np.random.uniform(LOWER_BOUND, UPPER_BOUND, size=dim).tolist()\n",
    "            final_queries.append([round(x, 6) for x in random_point])\n",
    "            logging.warning(f\"F{f_id}: EI optimization failed. Using random fallback: {final_queries[-1]}\")\n",
    "\n",
    "    # 5. Output Queries to JSON\n",
    "    if not os.path.exists(OUTPUTS_DIR):\n",
    "        os.makedirs(OUTPUTS_DIR)\n",
    "\n",
    "    output_path = os.path.join(OUTPUTS_DIR, OUTPUT_FILENAME)\n",
    "\n",
    "    try:\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(final_queries, f, indent=4)\n",
    "        logging.info(\"\\n\" + \"-\"*50)\n",
    "        logging.info(f\"SUCCESS: Generated {len(final_queries)} FINAL queries for Week 8.\")\n",
    "        logging.info(f\"File saved to: {output_path}\")\n",
    "        logging.info(f\"This is your submission file: {OUTPUT_FILENAME}\")\n",
    "        logging.info(\"-\"*50)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to write output JSON file: {e}\")\n",
    "\n",
    "    return final_queries\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    generated_queries = train_and_query_bbo()\n",
    "    if generated_queries:\n",
    "        print(\"\\n--- Queries for Week 08 Submission (week08_clean_inputs.json) ---\")\n",
    "        for i, q in enumerate(generated_queries):\n",
    "            # Print query F1 to F8, matching the required submission format\n",
    "            print(f\"F{i+1}: {q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171197bf-05d1-428d-bf06-89e3956286c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
