{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "539eba79-ab10-48c5-b199-c9a5fbca649e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9225e76-f577-40e8-8426-c8c45a9b6836",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up logging for cleaner output during execution\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_DATA_PATH = 'bbo_master_w02.csv' \n",
    "ADD_DATA_PATH = 'add_data'  # Assuming JSON files are in the current working directory\n",
    "OUTPUT_FILE_NAME = 'bbo_master_w03.csv'\n",
    "X_COLUMNS = ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8']\n",
    "N_FUNCTIONS = 8      # Number of unique Function IDs (1 through 8)\n",
    "POINTS_PER_FUNCTION_BLOCK = 10  # Each block in the initial 80 rows has 10 existing pts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "366cd0d2-9808-4659-a6e4-cb0adc5ddb4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "\n",
    "def load_new_queries_from_json_files(add_data_path, n_functions, x_columns):\n",
    "    \"\"\"\n",
    "    Loads the 8 NEW Week 2 data points from the JSON files.\n",
    "    Assumes the first 8 elements in both JSON arrays correspond to the new F1-F8 query results.\n",
    "    Returns a DataFrame of the 8 new rows (W2 queries).\n",
    "    \"\"\"\n",
    "    INPUTS_FILE = 'week02_clean_inputs.json'\n",
    "    OUTPUTS_FILE = 'week02_clean_outputs.json'\n",
    "    \n",
    "    try:\n",
    "        inputs_filepath = os.path.join(add_data_path, INPUTS_FILE)\n",
    "        with open(inputs_filepath, 'r') as f:\n",
    "            X_data = json.load(f)[:n_functions]\n",
    "            \n",
    "        outputs_filepath = os.path.join(add_data_path, OUTPUTS_FILE)\n",
    "        with open(outputs_filepath, 'r') as f:\n",
    "            Y_data = json.load(f)[:n_functions]\n",
    "            \n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"Required file not found: {e}. Please ensure the '{ADD_DATA_PATH}' directory and JSON files exist.\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading JSON files: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    total_points = len(X_data)\n",
    "    if total_points != n_functions or total_points != len(Y_data):\n",
    "        logging.error(f\"Expected {n_functions} new points after slicing, but found {total_points} or lengths do not match. Cannot proceed.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    logging.info(f\"Successfully loaded {total_points} new Week 2 query points (F1-F8).\")\n",
    "\n",
    "    new_rows = []\n",
    "    for i in range(total_points):\n",
    "        func_id = i + 1\n",
    "        X_values = X_data[i]\n",
    "        Y_value = Y_data[i]\n",
    "        \n",
    "        # Pad X list to 8 dimensions with NaN\n",
    "        X_padded = X_values + [np.nan] * (len(x_columns) - len(X_values))\n",
    "        \n",
    "        row_data = {\n",
    "            'Function ID': func_id,\n",
    "            'Y': Y_value,\n",
    "            **dict(zip(x_columns, X_padded))\n",
    "        }\n",
    "        new_rows.append(row_data)\n",
    "\n",
    "    return pd.DataFrame(new_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df922685-c1ea-421a-814e-62ec4e5fa7a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_bbo_master_w03():\n",
    "    \"\"\"\n",
    "    Creates the final W03 master data file using Structure 2 (Chronological Stacks): \n",
    "    Initial 80 rows -> Week 1 Queries (8 rows) -> Week 2 Queries (8 rows).\n",
    "    \"\"\"\n",
    "    logging.info(f\"--- Loading initial data from {BASE_DATA_PATH} (Expected 88 rows) ---\")\n",
    "    try:\n",
    "        df_master_w02 = pd.read_csv(BASE_DATA_PATH)\n",
    "        df_master_w02['Function ID'] = df_master_w02['Function ID'].astype(int)\n",
    "        \n",
    "        if len(df_master_w02) != 88:\n",
    "            logging.warning(f\"Expected 88 rows in {BASE_DATA_PATH}, but found {len(df_master_w02)}. Proceeding based on indices.\")\n",
    "        \n",
    "        logging.info(f\"Initial data points loaded: {len(df_master_w02)}.\")\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Base data file not found: {BASE_DATA_PATH}. Please ensure 'bbo_master_w02.csv' exists.\")\n",
    "        return\n",
    "    \n",
    "    # 1. Load the new W2 query results (8 rows)\n",
    "    df_w2_queries = load_new_queries_from_json_files(ADD_DATA_PATH, N_FUNCTIONS, X_COLUMNS)\n",
    "    \n",
    "    # FIX: Check if the DataFrame is empty using .empty\n",
    "    if df_w2_queries.empty: \n",
    "        logging.warning(\"No new query results were loaded. Stopping.\")\n",
    "        return\n",
    "        \n",
    "    # 2. Split the W02 file based on the Chronological Stacks structure\n",
    "    \n",
    "    # Block A (Rows 0-79): The 8 blocks of 10 initial points (80 rows)\n",
    "    INITIAL_BLOCK_SIZE = N_FUNCTIONS * POINTS_PER_FUNCTION_BLOCK\n",
    "    df_initial_blocks = df_master_w02.iloc[:INITIAL_BLOCK_SIZE].copy()\n",
    "    \n",
    "    # Block B (Rows 80-87): The final 8 rows which are the W1 queries (8 rows)\n",
    "    df_w1_queries = df_master_w02.iloc[INITIAL_BLOCK_SIZE:].copy()\n",
    "    \n",
    "    # df_w2_queries is Block C (8 rows)\n",
    "    \n",
    "    logging.info(f\"W02 Data Split: {len(df_initial_blocks)} Initial rows, {len(df_w1_queries)} W1 Query rows.\")\n",
    "\n",
    "    # 3. Concatenate the three parts in the desired chronological order: A -> B -> C\n",
    "    df_combined = pd.concat([df_initial_blocks, df_w1_queries, df_w2_queries], \n",
    "                            ignore_index=True, \n",
    "                            verify_integrity=True)\n",
    "    \n",
    "    # Final cleanup \n",
    "    df_combined['Function ID'] = df_combined['Function ID'].astype(int) \n",
    "    \n",
    "    # 4. Save the new master file\n",
    "    logging.info(\"---------------------------------------------\")\n",
    "    df_combined.to_csv(OUTPUT_FILE_NAME, index=False)\n",
    "    logging.info(f\"SUCCESS: New master data file '{OUTPUT_FILE_NAME}' created.\")\n",
    "    logging.info(f\"Total rows in the new file: {len(df_combined)} (Expected 96).\")\n",
    "    logging.info(\"Structure: 80 initial pts, then 8 W1 pts, then 8 W2 pts (Chronological Stacks).\")\n",
    "    logging.info(\"---------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3cd92d9-516d-4ee0-bb37-78b855095c50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: --- Loading initial data from bbo_master_w02.csv (Expected 88 rows) ---\n",
      "INFO: Initial data points loaded: 88.\n",
      "INFO: Successfully loaded 8 new Week 2 query points (F1-F8).\n",
      "INFO: W02 Data Split: 80 Initial rows, 8 W1 Query rows.\n",
      "INFO: ---------------------------------------------\n",
      "INFO: SUCCESS: New master data file 'bbo_master_w03.csv' created.\n",
      "INFO: Total rows in the new file: 96 (Expected 96).\n",
      "INFO: Structure: 80 initial pts, then 8 W1 pts, then 8 W2 pts (Chronological Stacks).\n",
      "INFO: ---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    create_bbo_master_w03()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f81073-cc7f-496d-af7b-6f393b5cf926",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
