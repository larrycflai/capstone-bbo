{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7db89c5b-5f31-4b8c-a2c8-5728b9810486",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "import logging\n",
    "import warnings\n",
    "import json\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2f62cbd2-8206-43cc-80f5-4f21f5db3410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress convergence warnings from GPR kernel optimization for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ea51b85b-d46a-4acb-928a-f358469b6432",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up logging for informative output\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ff7d083a-fe4b-4cfa-b433-9c8ab7229965",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "DATA_FILE = 'bbo_master_w03.csv'\n",
    "OUTPUT_QUERY_FILE = 'week03_queries.json'\n",
    "N_FUNCTIONS = 8\n",
    "RANDOM_STATE = 42\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# --- FUNCTION CONFIGURATION (As requested, using D=5, D=6, D=7) ---\n",
    "FUNCTION_CONFIG = {\n",
    "    1: {'D': 2, 'cols': ['X1', 'X2'], 'bounds': [(0, 1), (0, 1)]},\n",
    "    2: {'D': 2, 'cols': ['X1', 'X2'], 'bounds': [(0, 1), (0, 1)]},\n",
    "    3: {'D': 3, 'cols': ['X1', 'X2', 'X3'], 'bounds': [(0, 1), (0, 1), (0, 1)]},\n",
    "    4: {'D': 4, 'cols': ['X1', 'X2', 'X3', 'X4'], 'bounds': [(0, 1)] * 4},\n",
    "    5: {'D': 4, 'cols': ['X1', 'X2', 'X3', 'X4'], 'bounds': [(0, 1)] * 4}, \n",
    "    6: {'D': 5, 'cols': ['X1', 'X2', 'X3', 'X4', 'X5'], 'bounds': [(0, 1)] * 5}, \n",
    "    7: {'D': 6, 'cols': ['X1', 'X2', 'X3', 'X4', 'X5', 'X6'], 'bounds': [(0, 1)] * 6}, \n",
    "    8: {'D': 8, 'cols': ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8'], 'bounds': [(0, 1)] * 8},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ab740471-2aa6-4f33-aa92-604f6a8b58d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Loading Function (Must be defined once, outside the loop) ---\n",
    "def get_function_data(df: pd.DataFrame, func_id: int):\n",
    "    \"\"\"Filters the dataframe to get clean X and Y data for a specific function ID.\"\"\"\n",
    "    if func_id not in FUNCTION_CONFIG:\n",
    "        raise ValueError(f\"Function ID {func_id} not found in configuration.\")\n",
    "\n",
    "    config = FUNCTION_CONFIG[func_id]\n",
    "    feature_cols = config['cols']\n",
    "    \n",
    "    # Filter the initial function data\n",
    "    func_df = df[df['Function ID'] == func_id].copy()\n",
    "    \n",
    "    # CRITICAL STEP: Drop rows where any of the necessary features are NaN (using 'any' as we assume Y is always present if X isn't NaN)\n",
    "    func_df.dropna(subset=feature_cols, how='any', inplace=True)\n",
    "    \n",
    "    if func_df.empty:\n",
    "        raise ValueError(f\"Function ID {func_id} has no valid data after NaN removal.\")\n",
    "\n",
    "    X = func_df[feature_cols].values\n",
    "    Y = func_df['Y'].values.reshape(-1, 1) \n",
    "    \n",
    "    return X, Y, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2aa69ff0-f545-47d8-89fa-f3e5ce77e341",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Core Bayesian Optimisation Functions ---\n",
    "\n",
    "def expected_improvement(X, gpr, best_y):\n",
    "    \"\"\"\n",
    "    Calculates the Expected Improvement (EI) for a given point X for MINIMISATION.\n",
    "    \"\"\"\n",
    "    X = np.atleast_2d(X)\n",
    "    \n",
    "    # Predict the mean (mu) and standard deviation (sigma)\n",
    "    mu, sigma = gpr.predict(X, return_std=True)\n",
    "    \n",
    "    # CRITICAL FIX: Ensure mu and sigma are 2D column vectors (N, 1) for correct broadcasting\n",
    "    mu = mu.reshape(-1, 1)\n",
    "    sigma = sigma.reshape(-1, 1)\n",
    "    \n",
    "    # We seek improvement *below* the current minimum (best_y)\n",
    "    with np.errstate(divide='ignore'):\n",
    "        # Z = (best_y - mu) / sigma. All terms are now (N, 1) arrays or float (best_y)\n",
    "        Z = (best_y - mu) / sigma\n",
    "        \n",
    "        # EI formula for minimisation: sigma * (Z * Phi(Z) + phi(Z))\n",
    "        ei = sigma * (Z * norm.cdf(Z) + norm.pdf(Z))\n",
    "    \n",
    "    # If sigma is zero (e.g., at an observed point), EI must be zero\n",
    "    ei[sigma == 0.0] = 0.0\n",
    "    \n",
    "    return ei.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2db41c91-75da-496f-90fd-099fffdc679f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def propose_new_query(acquisition_func, gpr, best_y, config, n_restarts=50, n_random=1000):\n",
    "    \"\"\"\n",
    "    Finds the next point that maximizes the acquisition function using a hybrid \n",
    "    multi-start L-BFGS-B and random search strategy.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Minimisation objective (negative of the acquisition function)\n",
    "    def min_obj(X):\n",
    "        return -acquisition_func(X, gpr, best_y)\n",
    "\n",
    "    bounds_list = config['bounds']\n",
    "    d_dim = config['D']\n",
    "    \n",
    "    best_X = None\n",
    "    best_EI = -np.inf\n",
    "    \n",
    "    # 1. Multi-start L-BFGS-B optimization\n",
    "    for _ in range(n_restarts):\n",
    "        # Start optimization from a random point\n",
    "        X0 = np.random.uniform(np.array(bounds_list)[:, 0], np.array(bounds_list)[:, 1], size=d_dim)\n",
    "        \n",
    "        res = minimize(fun=min_obj,\n",
    "                       x0=X0,\n",
    "                       bounds=bounds_list,\n",
    "                       method='L-BFGS-B')\n",
    "        \n",
    "        # Update best result from local optimization\n",
    "        if res.success and -res.fun > best_EI:\n",
    "            best_EI = -res.fun\n",
    "            best_X = res.x\n",
    "            \n",
    "    # 2. Global Random Search (for better exploration)\n",
    "    X_random = np.random.uniform(np.array(bounds_list)[:, 0], np.array(bounds_list)[:, 1], size=(n_random, d_dim))\n",
    "    EI_random = acquisition_func(X_random, gpr, best_y)\n",
    "    \n",
    "    # Compare local optima with the best random point\n",
    "    if EI_random.max() > best_EI:\n",
    "        best_EI = EI_random.max()\n",
    "        best_X = X_random[np.argmax(EI_random)]\n",
    "            \n",
    "    # Fallback: If no successful optimization or better random point found, return a random query\n",
    "    if best_X is None:\n",
    "        logging.warning(f\"Optimization failed for D={d_dim}. Falling back to random query.\")\n",
    "        best_X = np.random.uniform(np.array(bounds_list)[:, 0], np.array(bounds_list)[:, 1], size=d_dim)\n",
    "    \n",
    "    return best_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ae2322fe-4488-458b-93c1-2bcfa6e0f891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_queries(df):\n",
    "    \"\"\"Generates one new query point for each function using Bayesian Optimisation.\"\"\"\n",
    "    \n",
    "    new_queries_list = []\n",
    "    \n",
    "    logging.info(\"--- Starting Query Generation for Week 03 ---\")\n",
    "    \n",
    "    for func_id in range(1, N_FUNCTIONS + 1):\n",
    "        \n",
    "        try:\n",
    "            # Use the dedicated function for robust data retrieval\n",
    "            X_data, Y_data, config = get_function_data(df, func_id)\n",
    "        except ValueError as e:\n",
    "            logging.error(f\"F{func_id}: Failed to load data: {e}\")\n",
    "            continue\n",
    "\n",
    "        d_dim = config['D']\n",
    "        N_train = len(X_data)\n",
    "        \n",
    "        # NOTE: Assumes Minimisation (lower Y is better)\n",
    "        best_y = Y_data.min() \n",
    "\n",
    "        logging.info(f\"F{func_id}: Training on {N_train} data points. Dimensionality (D): {d_dim}.\")\n",
    "\n",
    "        # Define the kernel and GPR model\n",
    "        kernel = C(1.0, (1e-3, 1e3)) * RBF(np.ones(d_dim), (1e-2, 1e2))\n",
    "        \n",
    "        gpr = GaussianProcessRegressor(\n",
    "            kernel=kernel, \n",
    "            alpha=1e-6, \n",
    "            n_restarts_optimizer=10, \n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "        gpr.fit(X_data, Y_data)\n",
    "        \n",
    "        # Propose a new query point using the hybrid strategy\n",
    "        new_X = propose_new_query(expected_improvement, gpr, best_y, config)\n",
    "        \n",
    "        # Construct the JSON output format: list of lists [[x1, x2, ...], ...]\n",
    "        # NOTE: Using list comprehension here ensures all numbers are cast to float for JSON compatibility\n",
    "        new_queries_list.append([float(x) for x in new_X])\n",
    "        \n",
    "        # Logging the result for confirmation\n",
    "        query_str = ', '.join([f'{val:.4f}' for val in new_X])\n",
    "        logging.info(f\"F{func_id}: New query calculated: [{query_str}]... | Best Y: {best_y:.4f}\")\n",
    "        \n",
    "    logging.info(\"--- Query Generation Complete ---\")\n",
    "    \n",
    "    # Print the full query output to console (as requested)\n",
    "    logging.info(\"\\n--- FULL WEEK 03 QUERY OUTPUT ---\")\n",
    "    logging.info(json.dumps(new_queries_list, indent=2))\n",
    "    logging.info(\"----------------------------------\\n\")\n",
    "    \n",
    "    # Save the queries as a JSON file\n",
    "    with open(OUTPUT_QUERY_FILE, 'w') as f:\n",
    "        json.dump(new_queries_list, f, indent=2)\n",
    "        \n",
    "    logging.info(f\"Successfully generated {len(new_queries_list)} new queries. Saved to {OUTPUT_QUERY_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7e8bd119-3139-4637-ac89-6f7d97109adc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # --- Step 0: Setup and Data Loading ---\n",
    "    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "    \n",
    "    try:\n",
    "        logging.info(f\"--- Loading data from {DATA_FILE} ---\")\n",
    "        df = pd.read_csv(DATA_FILE)\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"FATAL ERROR: Data file {DATA_FILE} not found.\")\n",
    "        return\n",
    "\n",
    "    # Initialise the list that will store all new queries\n",
    "    new_queries = []\n",
    "    \n",
    "    logging.info(\"--- Starting Week 03 Query Generation via Bayesian Optimisation ---\")\n",
    "    \n",
    "    # --- CORE BAYESIAN OPTIMISATION LOOP ---\n",
    "    for func_id in range(1, N_FUNCTIONS + 1):\n",
    "        info_prefix = f\"INFO: F{func_id}: \"\n",
    "        \n",
    "        try:\n",
    "            # Step 1: CORRECT DATA INGESTION & CLEANING\n",
    "            X, Y, feature_cols = get_function_data(df, func_id) \n",
    "            Y_best = Y.max()\n",
    "            \n",
    "            # Get dimensionality and bounds\n",
    "            D = X.shape[1]\n",
    "            bounds = FUNCTION_CONFIG[func_id]['bounds']\n",
    "\n",
    "            logging.info(f\"{info_prefix}Training on {len(X)} data points. Dimensionality (D): {D}.\")\n",
    "            \n",
    "            # Step 2: GPR INITIALISATION AND TRAINING\n",
    "            # D is used to define the correct size of the RBF length_scale vector\n",
    "            kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=[1.0] * D, length_scale_bounds=(1e-5, 1e4))\n",
    "            gpr = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, n_restarts_optimizer=20, random_state=RANDOM_STATE)\n",
    "            \n",
    "            gpr.fit(X, Y.ravel()) \n",
    "            \n",
    "            # Step 3: OPTIMISE ACQUISITION FUNCTION\n",
    "            # X_start must have the correct dimensionality D\n",
    "            X_start = np.random.uniform(0, 1, size=(D,))\n",
    "            \n",
    "            # Define the objective function (negative EI for minimization)\n",
    "            def objective(X_new):\n",
    "                # Now the call passes 5 arguments, matching the fixed EI signature\n",
    "                return -expected_improvement(X_new, X, gpr, Y_best, bounds)\n",
    "\n",
    "            # Perform the optimization\n",
    "            res = minimize(objective, X_start, bounds=bounds, method='L-BFGS-B')\n",
    "            \n",
    "            # Extract the new query point\n",
    "            new_query = res.x.round(4).tolist()\n",
    "            \n",
    "            logging.info(f\"{info_prefix}New query calculated: {new_query}... | Best Y: {Y_best:.4f}\")\n",
    "            \n",
    "            # Store the new query (Pad with NaNs for D < 8)\n",
    "            padding = [np.nan] * (8 - D)\n",
    "            new_queries.append([func_id] + new_query + padding)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Log any remaining errors\n",
    "            logging.error(f\"ERROR: F{func_id} General Error: {e}\")\n",
    "    \n",
    "    # --- Step 4: Save the New Queries ---\n",
    "    \n",
    "    # Define the columns for the output file\n",
    "    column_names = ['Function ID', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8']\n",
    "    \n",
    "    # Create the DataFrame\n",
    "    df_queries = pd.DataFrame(new_queries, columns=column_names)\n",
    "    \n",
    "    # Save the file\n",
    "    df_queries.to_csv(QUERIES_FILE, index=False)\n",
    "    \n",
    "    logging.info(\"--- Query Generation Complete ---\")\n",
    "    logging.info(f\"Successfully generated {len(df_queries)} new queries. Saved to {QUERIES_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8ab8e23a-6552-4e93-9bd6-9d04091e245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to load data and generate new queries.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(DATA_FILE)\n",
    "        logging.info(f\"Successfully loaded data from {DATA_FILE}. Total rows: {len(df)}\")\n",
    "        \n",
    "        generate_queries(df)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"ERROR: The data file '{DATA_FILE}' was not found. Please ensure it is uploaded.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during execution: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "927442ef-646d-455f-8c7e-f5c89d67afc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Successfully loaded data from bbo_master_w03.csv. Total rows: 96\n",
      "INFO: --- Starting Query Generation for Week 03 ---\n",
      "INFO: F1: Training on 12 data points. Dimensionality (D): 2.\n",
      "INFO: F1: New query calculated: [0.6614, 0.1852]... | Best Y: -0.0036\n",
      "INFO: F2: Training on 12 data points. Dimensionality (D): 2.\n",
      "INFO: F2: New query calculated: [0.1013, 0.3683]... | Best Y: -0.0656\n",
      "INFO: F3: Training on 12 data points. Dimensionality (D): 3.\n",
      "INFO: F3: New query calculated: [0.1546, 1.0000, 1.0000]... | Best Y: -0.3989\n",
      "INFO: F4: Training on 12 data points. Dimensionality (D): 4.\n",
      "INFO: F4: New query calculated: [1.0000, 0.7682, 0.7896, 0.3504]... | Best Y: -22.1083\n",
      "INFO: F5: Training on 1 data points. Dimensionality (D): 5.\n",
      "INFO: F5: New query calculated: [0.1803, 0.7241, 0.8122, 0.6791, 0.6069]... | Best Y: 30.7598\n",
      "INFO: F6: Training on 1 data points. Dimensionality (D): 6.\n",
      "INFO: F6: New query calculated: [0.7307, 0.6480, 0.8231, 0.6730, 0.5128, 0.1986]... | Best Y: -0.8670\n",
      "INFO: F7: Training on 12 data points. Dimensionality (D): 6.\n",
      "INFO: F7: New query calculated: [0.0000, 0.9332, 1.0000, 0.9753, 0.0000, 0.0125]... | Best Y: 0.0075\n",
      "INFO: F8: Training on 12 data points. Dimensionality (D): 8.\n",
      "INFO: F8: New query calculated: [0.8539, 0.4528, 0.9984, 0.0004, 0.1095, 0.3018, 0.0255, 0.7823]... | Best Y: 5.5922\n",
      "INFO: --- Query Generation Complete ---\n",
      "INFO: \n",
      "--- FULL WEEK 03 QUERY OUTPUT ---\n",
      "INFO: [\n",
      "  [\n",
      "    0.6613706110704144,\n",
      "    0.18519556786746794\n",
      "  ],\n",
      "  [\n",
      "    0.1013461398449633,\n",
      "    0.3682520811923021\n",
      "  ],\n",
      "  [\n",
      "    0.15463702437556107,\n",
      "    1.0,\n",
      "    1.0\n",
      "  ],\n",
      "  [\n",
      "    1.0,\n",
      "    0.7682383290257633,\n",
      "    0.7896264909850569,\n",
      "    0.35043004546005824\n",
      "  ],\n",
      "  [\n",
      "    0.1802901306156406,\n",
      "    0.7241359788326331,\n",
      "    0.8122395386087472,\n",
      "    0.6791288313348619,\n",
      "    0.6069222802265404\n",
      "  ],\n",
      "  [\n",
      "    0.7306609072679486,\n",
      "    0.6480314336331832,\n",
      "    0.8230611811425507,\n",
      "    0.6729813263239742,\n",
      "    0.512815619236686,\n",
      "    0.19858033564698754\n",
      "  ],\n",
      "  [\n",
      "    0.0,\n",
      "    0.9332017696196854,\n",
      "    1.0,\n",
      "    0.9752880010068485,\n",
      "    0.0,\n",
      "    0.01248197854009087\n",
      "  ],\n",
      "  [\n",
      "    0.8538519081332145,\n",
      "    0.4528127504114977,\n",
      "    0.9983815150970126,\n",
      "    0.00039969258932492924,\n",
      "    0.10948970841855665,\n",
      "    0.3017932429353404,\n",
      "    0.025507597953713157,\n",
      "    0.7823394577382508\n",
      "  ]\n",
      "]\n",
      "INFO: ----------------------------------\n",
      "\n",
      "INFO: Successfully generated 8 new queries. Saved to week03_queries.json\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9768d2b3-d712-43cd-ae49-e2f23dc3501c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056c5b88-109e-4237-b8ee-10046df1a8a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
