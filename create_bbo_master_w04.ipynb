{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75543918-2271-487d-894b-d76ca26a597b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "228b00e8-8499-4e23-878b-74ae4aa31a9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up logging for cleaner output during execution\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "474d08e1-bff3-45dc-88f5-9a0eda4f3f59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# Input: Master file from the previous week (Week 3)\n",
    "BASE_DATA_PATH = 'bbo_master_w03.csv' \n",
    "\n",
    "# The directory containing the new data files\n",
    "ADD_DATA_PATH = 'add_data'\n",
    "\n",
    "# Input: JSON files containing the new query results (Week 3 data)\n",
    "# NOTE: Using os.path.join to correctly reference files inside the 'add_data' folder\n",
    "X_INPUT_FILE = os.path.join(ADD_DATA_PATH, 'week03_clean_inputs.json')\n",
    "Y_OUTPUT_FILE = os.path.join(ADD_DATA_PATH, 'week03_clean_outputs.json')\n",
    "\n",
    "# Output: The final master file containing all data points (W0, W1, W2, W3)\n",
    "OUTPUT_FILE_NAME = 'bbo_master_w04.csv'\n",
    "\n",
    "# Column names used in the CSV files\n",
    "FUNCTION_ID_COL = 'Function ID'\n",
    "Y_COLUMN = 'Y'\n",
    "# Note: The number of X columns must match the maximum dimension (8)\n",
    "X_COLUMNS = [f'X{i}' for i in range(1, 9)]\n",
    "\n",
    "# Expected size checks (for verification)\n",
    "EXPECTED_NEW_ROWS = 8\n",
    "# 80 Initial + 8 W1 + 8 W2 = 96\n",
    "EXPECTED_INITIAL_ROWS = 96 \n",
    "# 96 + 8 W3 = 104\n",
    "EXPECTED_FINAL_ROWS = EXPECTED_INITIAL_ROWS + EXPECTED_NEW_ROWS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4de971e1-b67d-4df8-b65d-f7fc09b30295",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_json_data(file_path):\n",
    "    \"\"\"Loads data from a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Error: JSON file not found at {file_path}. Please ensure it is in the correct directory.\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        logging.error(f\"Error: Invalid JSON format in {file_path}.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e5ed89d-b38b-4ec0-b237-b295952b3172",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_query_df(inputs, outputs):\n",
    "    \"\"\"\n",
    "    Converts the list of lists (inputs) and list of floats (outputs)\n",
    "    into a structured DataFrame, ensuring all dimensions are present and correctly mapped\n",
    "    to the 8 functions (F1-F8).\n",
    "    \"\"\"\n",
    "    if len(inputs) != EXPECTED_NEW_ROWS or len(outputs) != EXPECTED_NEW_ROWS:\n",
    "        logging.error(f\"Input/Output count mismatch. Expected {EXPECTED_NEW_ROWS} pairs.\")\n",
    "        return None\n",
    "\n",
    "    # 1. Prepare data structure for 8 functions and 8 max dimensions\n",
    "    data = {col: [np.nan] * EXPECTED_NEW_ROWS for col in X_COLUMNS}\n",
    "    # Function IDs 1 to 8 correspond to the 8 data points in the list\n",
    "    data[FUNCTION_ID_COL] = list(range(1, EXPECTED_NEW_ROWS + 1)) \n",
    "    data[Y_COLUMN] = outputs\n",
    "\n",
    "    # 2. Map variable-length input vectors to the X1-X8 columns\n",
    "    for i, x_vec in enumerate(inputs):\n",
    "        for j, val in enumerate(x_vec):\n",
    "            # j is the dimension index (0 to N-1), so map to X_COLUMNS[j]\n",
    "            data[X_COLUMNS[j]][i] = val\n",
    "\n",
    "    # 3. Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    df[FUNCTION_ID_COL] = df[FUNCTION_ID_COL].astype(int)\n",
    "    \n",
    "    # Reorder columns to match the standard format: Function ID, X1..X8, Y\n",
    "    all_cols = [FUNCTION_ID_COL] + X_COLUMNS + [Y_COLUMN]\n",
    "    return df[all_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcea037b-97ff-4e70-b0ea-56a750d5473e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_bbo_master_w04():\n",
    "    \"\"\"Main function to load, append, and save the master BBO dataset.\"\"\"\n",
    "    logging.info(f\"--- Loading initial data from {BASE_DATA_PATH} (Expected {EXPECTED_INITIAL_ROWS} rows) ---\")\n",
    "    \n",
    "    # --- 1. Load existing master data ---\n",
    "    try:\n",
    "        master_df = pd.read_csv(BASE_DATA_PATH)\n",
    "        initial_rows = len(master_df)\n",
    "        logging.info(f\"Initial data points loaded: {initial_rows}.\")\n",
    "        if initial_rows != EXPECTED_INITIAL_ROWS:\n",
    "            logging.warning(f\"Warning: Loaded {initial_rows} rows, expected {EXPECTED_INITIAL_ROWS}. Continuing anyway.\")\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Base file not found at {BASE_DATA_PATH}. Cannot proceed.\")\n",
    "        return\n",
    "    except pd.errors.EmptyDataError:\n",
    "        logging.error(\"Base file is empty. Cannot proceed.\")\n",
    "        return\n",
    "    \n",
    "    # --- 2. Load and validate new query data ---\n",
    "    query_inputs = load_json_data(X_INPUT_FILE)\n",
    "    query_outputs = load_json_data(Y_OUTPUT_FILE)\n",
    "    \n",
    "    if query_inputs is None or query_outputs is None:\n",
    "        logging.error(\"Failed to load new query JSON files. Master file not created.\")\n",
    "        return\n",
    "        \n",
    "    if len(query_inputs) != EXPECTED_NEW_ROWS or len(query_outputs) != EXPECTED_NEW_ROWS:\n",
    "        logging.error(f\"Loaded {len(query_inputs)} inputs and {len(query_outputs)} outputs,\" +\n",
    "                      \"expected {EXPECTED_NEW_ROWS} of each. Master file not created.\")\n",
    "        return\n",
    "        \n",
    "    logging.info(f\"Successfully loaded {EXPECTED_NEW_ROWS} new Week 3 query points from '{ADD_DATA_PATH}' (F1-F8).\")\n",
    "    \n",
    "    # --- 3. Create the DataFrame for the new query data ---\n",
    "    # Added try/except to prevent NameError if an unhandled exception occurs during assignment\n",
    "    try:\n",
    "        query_df = create_query_df(query_inputs, query_outputs)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Critical error creating query DataFrame: {e}. Cannot proceed.\")\n",
    "        return\n",
    "\n",
    "    if query_df is None:\n",
    "        logging.error(\"Error creating query DataFrame. Master file not created.\")\n",
    "        return\n",
    "    \n",
    "    # --- 4. Concatenate and save ---\n",
    "    # Stack the new query data below the existing master data\n",
    "    new_master_df = pd.concat([master_df, query_df], ignore_index=True)\n",
    "    \n",
    "    # Save the final file, using an empty string for NaN values in the output CSV\n",
    "    new_master_df.to_csv(OUTPUT_FILE_NAME, index=False, na_rep='')\n",
    "    \n",
    "    # --- 5. Final Verification ---\n",
    "    final_rows = len(new_master_df)\n",
    "    logging.info(\"INFO: ---------------------------------------------\")\n",
    "    logging.info(f\"SUCCESS: New master data file '{OUTPUT_FILE_NAME}' created.\")\n",
    "    logging.info(f\"Total rows in the new file: {final_rows} (Expected {EXPECTED_FINAL_ROWS}).\")\n",
    "    logging.info(f\"Verification: There are now {final_rows // 8} data points for each of the 8 functions.\")\n",
    "    logging.info(\"INFO: ---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abf6a2b4-09a7-4ff1-babf-c78bd9ef4526",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: --- Loading initial data from bbo_master_w03.csv (Expected 96 rows) ---\n",
      "INFO: Initial data points loaded: 96.\n",
      "INFO: Successfully loaded 8 new Week 3 query points from 'add_data' (F1-F8).\n",
      "INFO: INFO: ---------------------------------------------\n",
      "INFO: SUCCESS: New master data file 'bbo_master_w04.csv' created.\n",
      "INFO: Total rows in the new file: 104 (Expected 104).\n",
      "INFO: Verification: There are now 13 data points for each of the 8 functions.\n",
      "INFO: INFO: ---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    create_bbo_master_w04()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fef7bc7-a8a8-421c-aae5-47611442151e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
