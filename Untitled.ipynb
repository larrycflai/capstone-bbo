{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fde0130-d012-49d9-bde3-6d78da4a9f72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: --- Processing Function F1 (2D) ---\n",
      "ERROR: Error processing Function F1: Input X still contains NaN within columns X1-X2. Returning random query.\n",
      "INFO: --- Processing Function F2 (2D) ---\n",
      "ERROR: Error processing Function F2: Input X still contains NaN within columns X1-X2. Returning random query.\n",
      "INFO: --- Processing Function F3 (3D) ---\n",
      "ERROR: Error processing Function F3: Input X still contains NaN within columns X1-X3. Returning random query.\n",
      "INFO: --- Processing Function F4 (4D) ---\n",
      "ERROR: Error processing Function F4: Input X still contains NaN within columns X1-X4. Returning random query.\n",
      "INFO: --- Processing Function F5 (4D) ---\n",
      "ERROR: Error processing Function F5: Input X still contains NaN within columns X1-X4. Returning random query.\n",
      "INFO: --- Processing Function F6 (5D) ---\n",
      "ERROR: Error processing Function F6: Input X still contains NaN within columns X1-X5. Returning random query.\n",
      "INFO: --- Processing Function F7 (6D) ---\n",
      "ERROR: Error processing Function F7: Input X still contains NaN within columns X1-X6. Returning random query.\n",
      "INFO: --- Processing Function F8 (8D) ---\n",
      "ERROR: Error processing Function F8: Input X still contains NaN within columns X1-X8. Returning random query.\n",
      "INFO: \n",
      "--------------------------------------------------\n",
      "INFO: SUCCESS: Generated 8 FINAL queries for Week 7 (Rounds 0-16).\n",
      "INFO: File saved to: 'add_data/week07_clean_inputs.json'. This is the file to submit.\n",
      "INFO: --------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "import logging\n",
    "import warnings\n",
    "import json\n",
    "import os\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "# Import necessary components from scikit-learn for GPR\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, ConstantKernel as C, WhiteKernel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_FILE_PATH = 'bbo_master_w06.csv' # Adjust if your master file name has changed\n",
    "ADD_DATA_DIR = 'add_data'\n",
    "OUTPUT_FILE_PATH = os.path.join(ADD_DATA_DIR, 'week07_clean_inputs.json')\n",
    "\n",
    "# Define the true dimensionality (D) for each of the 8 functions\n",
    "FUNCTION_DIMENSIONS = {\n",
    "    1: 2, 2: 2, 3: 3, 4: 4,\n",
    "    5: 4, 6: 5, 7: 6, 8: 8\n",
    "}\n",
    "\n",
    "# Set up logging for cleaner output\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "# Suppress GPR numerical warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "\n",
    "\n",
    "def expected_improvement(X, gp, f_best):\n",
    "    \"\"\"Acquisition function: Expected Improvement (EI)\"\"\"\n",
    "    # X must be reshaped if it's a single point (1D array to 2D array)\n",
    "    X = np.atleast_2d(X)\n",
    "    \n",
    "    # Calculate mean and standard deviation\n",
    "    mu, sigma = gp.predict(X, return_std=True)\n",
    "    sigma = sigma.reshape(-1, 1) # Ensure sigma is 2D\n",
    "    \n",
    "    # Calculate difference and z-score\n",
    "    diff = mu - f_best\n",
    "    with np.errstate(divide='ignore'): # Ignore division by zero warnings\n",
    "        Z = diff / sigma\n",
    "        \n",
    "    # Calculate Expected Improvement (EI)\n",
    "    # The EI formula is: (mu - f_best) * CDF(Z) + sigma * PDF(Z)\n",
    "    ei = diff * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    \n",
    "    # Optimization aims for minimisation, so we return the negative EI\n",
    "    return -ei.flatten()\n",
    "\n",
    "\n",
    "def process_function(function_id: int, df_all: pd.DataFrame) -> Tuple[List[float], float]:\n",
    "    \"\"\"\n",
    "    Optimises the next query point for a single function using GPR and EI.\n",
    "    \n",
    "    This function includes the crucial fix for NaN values by selecting only \n",
    "    the relevant X columns (X1 to XD).\n",
    "    \"\"\"\n",
    "    D = FUNCTION_DIMENSIONS.get(function_id)\n",
    "    if D is None:\n",
    "        logging.error(f\"Missing dimension for Function F{function_id}. Skipping.\")\n",
    "        return [], 0.0\n",
    "\n",
    "    logging.info(f\"--- Processing Function F{function_id} ({D}D) ---\")\n",
    "    \n",
    "    # 1. Prepare Data\n",
    "    df_func = df_all[df_all['Function ID'] == function_id].copy()\n",
    "    \n",
    "    # X_columns are the columns X1, X2, ..., X8\n",
    "    X_cols = [f'X{i}' for i in range(1, 9)]\n",
    "    X_train_raw = df_func[X_cols].values\n",
    "    Y_train = df_func['Y'].values\n",
    "\n",
    "    # CRUCIAL FIX: Filter out the NaN columns based on the true dimension (D)\n",
    "    # This selects X1, ..., XD and avoids feeding NaNs to the GPR.\n",
    "    X_train_clean = X_train_raw[:, :D]\n",
    "    \n",
    "    # Check for NaNs just in case the data frame was modified elsewhere\n",
    "    if np.isnan(X_train_clean).any():\n",
    "         # This should now only trigger if there are NaNs within the relevant columns\n",
    "         logging.error(f\"Error processing Function F{function_id}: Input X still contains NaN within columns X1-X{D}. Returning random query.\")\n",
    "         # Fallback to a random query if cleaning somehow failed\n",
    "         return np.random.uniform(0.0, 1.0, D).tolist(), 0.0\n",
    "\n",
    "    logging.info(f\"Loaded {len(X_train_clean)} points for F{function_id}. Fixed Dimension D={D} enforced.\")\n",
    "\n",
    "    # Scale the Y values for better GPR performance\n",
    "    scaler = StandardScaler()\n",
    "    Y_scaled = scaler.fit_transform(Y_train.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # The maximum (best) value in the scaled space\n",
    "    f_best_scaled = np.max(Y_scaled)\n",
    "    logging.info(f\"Best known MAXIMUM (Scaled Y) for F{function_id}: {f_best_scaled:.4f}\")\n",
    "\n",
    "    # 2. Train GPR Model\n",
    "    # Using Matern kernel (v=2.5) with a flexible length-scale and constant amplitude\n",
    "    kernel = C(1.0, (1e-3, 1e3)) * Matern(length_scale=[1.0]*D, length_scale_bounds=(1e-2, 1e3), nu=2.5) \\\n",
    "             + WhiteKernel(noise_level=0.1, noise_level_bounds=(1e-5, 1e1))\n",
    "    \n",
    "    gp = GaussianProcessRegressor(\n",
    "        kernel=kernel, \n",
    "        alpha=1e-6, \n",
    "        n_restarts_optimizer=15, \n",
    "        normalize_y=False # Already scaled manually\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        gp.fit(X_train_clean, Y_scaled)\n",
    "        # logging.info(f\"GPR Model Trained. Log-Marginal-Likelihood: {gp.log_marginal_likelihood(gp.kernel_.theta):.15f}\")\n",
    "    except ValueError as e:\n",
    "        # If GPR fit fails, return a random query\n",
    "        logging.error(f\"Error fitting GPR for F{function_id}: {e}. Returning random query.\")\n",
    "        return np.random.uniform(0.0, 1.0, D).tolist(), 0.0\n",
    "\n",
    "    # 3. Optimise Acquisition Function (Expected Improvement)\n",
    "    \n",
    "    # We use a multi-start optimisation approach to find the global minimum of -EI\n",
    "    n_restarts = 10\n",
    "    best_query = None\n",
    "    max_ei = -np.inf\n",
    "\n",
    "    # Define the objective function wrapper for minimizer\n",
    "    objective = lambda x: expected_improvement(x, gp, f_best_scaled)\n",
    "    # Define bounds for the optimisation (0.0 to 1.0 for all D dimensions)\n",
    "    bounds = [(0.0, 1.0)] * D\n",
    "    \n",
    "    for i in range(n_restarts):\n",
    "        # Start search from a random point\n",
    "        x0 = np.random.uniform(0.0, 1.0, D)\n",
    "        \n",
    "        # Use L-BFGS-B minimisation algorithm\n",
    "        res = minimize(\n",
    "            objective, \n",
    "            x0, \n",
    "            method='L-BFGS-B', \n",
    "            bounds=bounds\n",
    "        )\n",
    "        \n",
    "        # If a better point (lower negative EI, thus higher positive EI) is found\n",
    "        if res.success and -res.fun[0] > max_ei:\n",
    "            max_ei = -res.fun[0]\n",
    "            best_query = res.x\n",
    "            \n",
    "    logging.info(f\"Optimal EI found: {max_ei:.4f}\")\n",
    "\n",
    "    # 4. Format Output\n",
    "    if best_query is None:\n",
    "        # Fallback if optimisation fails completely\n",
    "        logging.warning(f\"Optimisation failed for F{function_id}. Returning random query.\")\n",
    "        return np.random.uniform(0.0, 1.0, D).tolist(), 0.0\n",
    "\n",
    "    # Clip values to ensure they are strictly within [0, 1] for submission\n",
    "    final_query = np.clip(best_query, 0.0, 1.0).tolist()\n",
    "    logging.info(f\"F{function_id} Proposed Query (D={D}): \" + ' | '.join([f'{x:.6f}' for x in final_query]))\n",
    "\n",
    "    return final_query, max_ei\n",
    "\n",
    "\n",
    "def run_optimisation():\n",
    "    \"\"\"Main function to run the optimisation loop for all 8 functions.\"\"\"\n",
    "    \n",
    "    if not os.path.exists(ADD_DATA_DIR):\n",
    "        os.makedirs(ADD_DATA_DIR)\n",
    "        \n",
    "    try:\n",
    "        df_master = pd.read_csv(DATA_FILE_PATH)\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Could not find the master data file at: {DATA_FILE_PATH}. Please ensure it exists.\")\n",
    "        return\n",
    "\n",
    "    # Ensure all inputs are treated as numbers, errors will be handled by the check inside process_function\n",
    "    df_master.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    # Iterate over all 8 functions\n",
    "    all_queries = []\n",
    "    \n",
    "    for func_id in range(1, 9):\n",
    "        new_query, ei_value = process_function(func_id, df_master)\n",
    "        \n",
    "        # Pad the query with 0.0s up to D=8 for submission format\n",
    "        # IMPORTANT: The API only uses the first D dimensions, but the JSON must be padded to D=8 for all entries.\n",
    "        padded_query = new_query + [0.0] * (8 - len(new_query))\n",
    "        all_queries.append(padded_query)\n",
    "\n",
    "    # 5. Save Results to JSON\n",
    "    logging.info(\"\\n--------------------------------------------------\")\n",
    "    logging.info(\"SUCCESS: Generated 8 FINAL queries for Week 7 (Rounds 0-16).\")\n",
    "    logging.info(f\"File saved to: '{OUTPUT_FILE_PATH}'. This is the file to submit.\")\n",
    "    \n",
    "    with open(OUTPUT_FILE_PATH, 'w') as f:\n",
    "        # Format the JSON data to match the required submission format (array of arrays)\n",
    "        json.dump(all_queries, f, indent=2, allow_nan=False)\n",
    "        \n",
    "    logging.info(\"--------------------------------------------------\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_optimisation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
